# Slurm 사용법!

## Slurm workload manager이 무엇인가요?

Slurm은 오픈 소스 **리눅스 클러스터 작업 스케줄러 및 워크로드 매니저**입니다. 

컴공과 클러스터를 쓸 때, 많은 계산량을 요구하는 작업에 대해 여러 명의 인원이 같이 써야 하므로 이 도구를 사용하게 될 거에요.

https://slurm.schedmd.com/documentation.html 공식 문서 참고!

### Architecture

##### **slurmctld** 

중앙 관리자, 리소스와 작업을 모니터링한다. 문제 발생 시 백업 매니저를 통해 작업을 관리할 수 있다.

##### **slurmd**  

각각 서버(노드)가 가진 Daemon으로, 원격 셸의 역할을 한다. 작업 대기, 실행, 상태 출력, 계층 간 통신 등을 제공한다.

##### **slurmdbd**  

Slurm Database Daemon, 여러 slurm 관리 클러스터에 대한 정보를 기록하는 단일 데이터베이스로 쓰일 수 있다.

##### **slurmrestd**  

Slurm REST API Daemon, REST API를 통해 Slurm과 상호작용한다.

### 사용자 도구

**srun** : 작업을 시작하기 위한 도구

**scancel** : 대기 중이거나 실행 중인 작업을 종료합니다.

**sinfo** : 시스템 현황을 출력합니다.

**squeue** : 현재 진행되고 있는 작업들을 열람합니다.

**sacct** : 현재 작업, 실행 중이거나 완료된 작업 스텝에 대한 정보를 열람합니다.

**sview** : Network Topology를 포함한 시스템 및 작업 상태를 그래픽으로 보고한다.

**sacctmgr** : 데이터베이스를 관리한다. 클러스터, 유효한 사용자, 유요한 계정 등을 식별하는 데 사용할 수 있다.

## 사전지식

1. 리눅스 기본 명령어 : https://itholic.github.io/linux-basic-command/
2. shell script : https://twpower.github.io/131-simple-shell-script-syntax



## 컴공과 클러스터의 slurm 스크립트 갖다쓰기

클러스터의 /opt/ohpc/pub/apps/SLURM-Batch-Training 폴더를 보면, 이미 누군가 잘 써 놓은 slurm 트레이닝 배치 파일이 있다. 이 디렉토리에 있는 것을 이용해 slurm을 이용해 클러스터의 GPU를 쓸 수 있고, 텐서플로우를 테스트할 수 있다!

### Directory 구성

01.GPU

​	conda-pytorch-1gpu-node-n5.slurm.sh 

​	conda-tf-multi-2gpu.slurm

​	nbody-single-node-n13.slurm.sh

​	nbody-single-node-n15.slurm.sh

​	nbody-single-node-n18.slurm.sh

​	nccl-multi-gpu.slurm.sh

​	nccl-single-gpu-slurm.sh

​	tf-2gpu.slurm.sh

​	tf-4gpu.slurm.sh

02.CPU

​	nest

​	Sleep(sleepy.slurm.sh)

### 복사해서 사용하기

바로 slurm 스크립트를 복사해서 가져다 쓸 수 있다.

```
$ cp -r /opt/ohpc/pub/apps/SLURM-Batch-Training/ ~
$ cp -r /opt/ohpc/pub/apps/TensorFlow-2.x-Tutorials/ ~
$ cd SLURM-Batch-Training
$ cd 02.CPU
$ cd Sleep/
$ sbatch sleepy.slurm.sh
$ squeue
$ cd ../01.GPU
$ sbatch tf-single-gpu.slurm.sh
$ squeue
$ sinfo
```

.sh shell script를 sbatch로 실행시키면 된다! 완전 간단하다. 

여기서 쓰인 명령어를 알아보자면,

cp : 파일이나 폴더를 복사한다. -r 을 넣으면 폴더나 디렉토리를 복사하는 명령이 된다!

cd : 경로를 이동한다.

sbatch : 배치 스크립트 파일을 실행시킨다.

squeue : 현재 진행되고 있는 작업 목록을 출력한다.

## 명령줄 설명

#### sinfo

slurm에서 sinfo는 slurm의 노드 및 파티션 정보를 조회하는 명령어이다.

https://slurm.schedmd.com/sinfo.html

과 클러스터의 경우 sinfo를 출력하면 다음과 같은 내용이 출력될 것이다.

![image-20200724001056646](C:\Users\stkd3\AppData\Roaming\Typora\typora-user-images\image-20200724001056646.png)



18개의 노드로 구성되어 있는데, n1-n13까지는 gpu titanXP 4장, n14는 gpu card가 없고, n15에 RTX:8장, n16은 gpu 없고, n17, n18에 gpu 4장이 할당되어 있다.

![image-20200724164140101](C:\Users\stkd3\AppData\Roaming\Typora\typora-user-images\image-20200724164140101.png)

#### squeue

![image-20200724164212856](C:\Users\stkd3\AppData\Roaming\Typora\typora-user-images\image-20200724164212856.png)

#### srun (or sbatch)

srun 명령어의 arguments들을 통해 어떤 자원을 사용할지를 결정할 수 있다.

https://slurm.schedmd.com/srun.html

```
srun -p gpu-titanxp -N 1 -n 1 -t 00:30:00 --gres=gpu:1 --pty /bin/bash -l
```

-p PARTITION : partition 설정

-N : CPU가 가진 node의 수

-n : CPU의 개수

--gres=gpu: GPU CARD의 개수

![image-20200724190724358](C:\Users\stkd3\AppData\Roaming\Typora\typora-user-images\image-20200724190724358.png)

ex)

```
$ srun -p gpu-titanxp -N 1 -n 1 -t 00:30:00 --gres=gpu:1 --pty /bin/bash -l
```

: gpu-titanxp 리소스 자원 중에서 node 1대의 cpu 1개와 gpu card 1장을 사용하겠다고 정의하여 작업을 수행하는 방법. 최대 작업시간은 30분.



#### sbatch + 배치 스크립트

```bash
$ cat 02.CPU/sleepy.slurm.sh

#!/bin/bash
#SBATCH -J sleepy : 작업 이름 지정
#SBATCH -o sleepy.%j.out : 작업 완료된 후 출력되는 이름
#SBATCH -t 00:05:30 작업시간 범위
#SBATCH --nodes=1 : 작업의 노드 수 지정
#SBATCH --ntasks = 2 : 작업의 갯수 지정(작업이 2개)
#SBATCH --tasks-per-node = 2 : 노드당 작업되는 작업 수 지정
#SBATCH --cpus per-task = 1 : 작업당 cpu 1개씩 할당
set echo on
cd $SLURM_SUBMIT_DIR
srun -l /bin/hostname
srun -l /bin/pwd
srun -l /bin/date
module purge
#충돌을 방지하기 위해 기존의 환경변수 삭제
module lad postech
#기본 환경변수 지정
date
sleep 120
date
squeue -- job $SLURM_JOBID
```

![image-20200724185253035](C:\Users\stkd3\AppData\Roaming\Typora\typora-user-images\image-20200724185253035.png)

이미 만들어져 있는 배치 스크립트를 수정하거나 볼 수 있다. 실행은 sbatch sleepy.slurm.sh로 한다음 squeue와 scontrol, sinfo로 정보를 확인하자.



#### salloc

srun과 같은데, salloc은 작업 할당 (노드 세트)을 확보하고 srun 명령을 실행한 다음 명령이 완료되면 할당을 해제한다. 또한 bash 명령어를 주지 않습니다.

https://slurm.schedmd.com/salloc.html

```bash
$ salloc --ntasks=8 --time=1:00:00 --mem-per-cpu=2GB
```

```bash
$ salloc --nodelist=n1 --ntasks=6 --time=1:00:00 --mem-per-cpu=2GB
```







interactive test

